{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1KBrrxfn7g_7TpqEGUNw2oibWFebbIGXY",
      "authorship_tag": "ABX9TyNuEVTXg+I9smUjljOeNNBq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MsYGtWSNxhq",
        "outputId": "46d0180e-adc6-4bdb-d37d-a4a0f3a47766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-02 14:24:10--  https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-02T15%3A18%3A00Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-02T14%3A17%3A03Z&ske=2025-12-02T15%3A18%3A00Z&sks=b&skv=2018-11-09&sig=%2BNMYcMwnvZkPV5ks47IPEnTXLfZ3KXcRdDGLLI3%2BZPE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDY4NzI1MCwibmJmIjoxNzY0Njg1NDUwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.EryvLBi5TmC0dZYM1a-W5nT07ddV5bhtdI0GcTjtrQA&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-02 14:24:11--  https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-02T15%3A18%3A00Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-02T14%3A17%3A03Z&ske=2025-12-02T15%3A18%3A00Z&sks=b&skv=2018-11-09&sig=%2BNMYcMwnvZkPV5ks47IPEnTXLfZ3KXcRdDGLLI3%2BZPE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDY4NzI1MCwibmJmIjoxNzY0Njg1NDUwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.EryvLBi5TmC0dZYM1a-W5nT07ddV5bhtdI0GcTjtrQA&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102516572 (98M) [application/octet-stream]\n",
            "Saving to: ‘data.zip.1’\n",
            "\n",
            "data.zip.1          100%[===================>]  97.77M   314MB/s    in 0.3s    \n",
            "\n",
            "2025-12-02 14:24:11 (314 MB/s) - ‘data.zip.1’ saved [102516572/102516572]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "475yFH_zN3Tr",
        "outputId": "5df52ce7-c927-4870-812d-8ac7258a16ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "replace data/test/curly/03312ac556a7d003f7570657f80392c34.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "bEnZaNhfN7Zz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "tnQzBjnxevW_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlnafQr7ukWg",
        "outputId": "676878d5-ceb0-403a-9ecf-f8c8b8d1baeb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    # 3 input image channels (RGB), 32 output channels, 3x3 square convolution kernel\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.vectorize = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(32 * 99 * 99, 64)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.output = nn.Linear(64, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.relu(self.conv1(x))      # Conv + ReLU\n",
        "      x = self.pool(x)                  # Max pooling\n",
        "      x = self.vectorize(x)             # Flatten to 1D\n",
        "      x = self.relu2(self.fc1(x))       # Linear + ReLU\n",
        "      x = self.output(x)                # Final output (raw logits)\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "TbUqp1I956rT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "loII12xu_RRO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QTLwWuGAaEa",
        "outputId": "7eeea5b7-ae7f-427a-93cd-3859b7d7a4cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyModel(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (vectorize): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=313632, out_features=64, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2 - Total parameters\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp83UuPjBpAo",
        "outputId": "3f003c70-e3f8-4c64-f54c-327cb5b771b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 20073473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generators and Training\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ) # ImageNet normalization\n",
        "])"
      ],
      "metadata": {
        "id": "Cx4bXPfOBysG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply transforms to dataset\n",
        "train_dataset = datasets.ImageFolder('/content/data/train', transform=train_transforms)\n",
        "\n",
        "# Split train_dataset into train/validation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "train_subset, validation_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
        "validation_loader = DataLoader(validation_subset, batch_size=20, shuffle=False)"
      ],
      "metadata": {
        "id": "ZMFuhrOERKYc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "TrAr2u0aP95K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['acc'].append(epoch_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validation_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(validation_subset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history['val_loss'].append(val_epoch_loss)\n",
        "    history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b30AvqTdOA5A",
        "outputId": "224fd612-d33b-44e2-cf28-f6592ef873e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6459, Acc: 0.6200, Val Loss: 0.6745, Val Acc: 0.5875\n",
            "Epoch 2/10, Loss: 0.5688, Acc: 0.6887, Val Loss: 0.5347, Val Acc: 0.7562\n",
            "Epoch 3/10, Loss: 0.5349, Acc: 0.6963, Val Loss: 0.4803, Val Acc: 0.7750\n",
            "Epoch 4/10, Loss: 0.4737, Acc: 0.7688, Val Loss: 0.4059, Val Acc: 0.7937\n",
            "Epoch 5/10, Loss: 0.4177, Acc: 0.7975, Val Loss: 0.3877, Val Acc: 0.8000\n",
            "Epoch 6/10, Loss: 0.3725, Acc: 0.8275, Val Loss: 0.5141, Val Acc: 0.7375\n",
            "Epoch 7/10, Loss: 0.3324, Acc: 0.8400, Val Loss: 0.2517, Val Acc: 0.9250\n",
            "Epoch 8/10, Loss: 0.2679, Acc: 0.8862, Val Loss: 0.2308, Val Acc: 0.9187\n",
            "Epoch 9/10, Loss: 0.1824, Acc: 0.9287, Val Loss: 0.1251, Val Acc: 0.9812\n",
            "Epoch 10/10, Loss: 0.1484, Acc: 0.9425, Val Loss: 0.1209, Val Acc: 0.9563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3: Median of training accuracy for all the epochs\n",
        "median = np.median(history['acc'])\n",
        "print(f\"Median is: {median:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uD802ePXoXe",
        "outputId": "09e8065a-c1a0-4d4e-ff9e-0ef7789e02b2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median is: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 4: What is the standard deviation of training loss for all the epochs for this model?\n",
        "std_dev = np.std(history['loss'])\n",
        "print(f\"Standard deviation is: {std_dev:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "japd5B6OU9yW",
        "outputId": "ef477969-e443-4d84-eb02-19be1f52a261"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard deviation is: 0.157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "\n",
        "transforms.RandomRotation(50),\n",
        "transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "transforms.RandomHorizontalFlip(),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D93W1V26Y8_l",
        "outputId": "51894fcc-f6d3-4ee9-d9c4-461fe9ddbdbd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(RandomHorizontalFlip(p=0.5),)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "augmented_history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(10, num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    augmented_history['loss'].append(epoch_loss)\n",
        "    augmented_history['acc'].append(epoch_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validation_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(validation_subset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    augmented_history['val_loss'].append(val_epoch_loss)\n",
        "    augmented_history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ychj_h3ZCz8",
        "outputId": "e2a58d9f-f98b-4116-f6f1-7d2cefcdccc9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20, Loss: 0.1039, Acc: 0.9625, Val Loss: 0.1870, Val Acc: 0.9125\n",
            "Epoch 12/20, Loss: 0.1472, Acc: 0.9487, Val Loss: 0.1637, Val Acc: 0.9125\n",
            "Epoch 13/20, Loss: 0.0670, Acc: 0.9825, Val Loss: 0.0473, Val Acc: 0.9938\n",
            "Epoch 14/20, Loss: 0.0256, Acc: 0.9988, Val Loss: 0.0209, Val Acc: 1.0000\n",
            "Epoch 15/20, Loss: 0.0145, Acc: 1.0000, Val Loss: 0.0132, Val Acc: 1.0000\n",
            "Epoch 16/20, Loss: 0.0100, Acc: 1.0000, Val Loss: 0.0089, Val Acc: 1.0000\n",
            "Epoch 17/20, Loss: 0.0076, Acc: 1.0000, Val Loss: 0.0070, Val Acc: 1.0000\n",
            "Epoch 18/20, Loss: 0.0059, Acc: 1.0000, Val Loss: 0.0064, Val Acc: 1.0000\n",
            "Epoch 19/20, Loss: 0.0051, Acc: 1.0000, Val Loss: 0.0050, Val Acc: 1.0000\n",
            "Epoch 20/20, Loss: 0.0043, Acc: 1.0000, Val Loss: 0.0044, Val Acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5: What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
        "mean = np.mean(augmented_history['loss'])\n",
        "print(f\"Mean is: {mean:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xMdUZXFZ4e0",
        "outputId": "1fb71eeb-72a0-4a0b-da93-74f6d3765e34"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean is: 0.039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?\n",
        "mean = np.mean(augmented_history['acc'][5:])\n",
        "print(f\"Mean is: {mean:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWPWvNU4aw_2",
        "outputId": "d9add23d-7aa9-4dd5-ae75-d559bb30b003"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean is: 1.000\n"
          ]
        }
      ]
    }
  ]
}